import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import KFold, train_test_split
import os
import time
import joblib
import itertools
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

# 创建输出目录
output_dir = "F:\\Machine leaning_SHAP\\LightGBM\\2"
os.makedirs(output_dir, exist_ok=True)

# 设置中文显示
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# 读取数据
print("1. 读取数据")
data = pd.read_excel("F:\\Machine leaning_SHAP\\数据集.xlsx")
print(f"数据量: {data.shape[0]}行")

# 定义特征和目标变量
features = ["TA", "RH", "P", "SP", "SD", "ATM", "PAR", "TS", "shallow_SWC", "deep_SWC"]
target = "NEP"
X, y = data[features], data[target]

# 划分数据集
print("\n2. 划分数据集")
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=5856, random_state=42, shuffle=True)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=7344, random_state=42, shuffle=True)
print(f"训练集: {X_train.shape[0]} | 验证集: {X_val.shape[0]} | 测试集: {X_test.shape[0]}")

# 数据归一化
print("\n3. 数据归一化")
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 合并训练集和验证集用于交叉验证
X_train_val = np.vstack((X_train_scaled, X_val_scaled))
y_train_val = pd.concat([y_train, y_val])

# 网格搜索粗调
print("\n4. 网格搜索粗调")
start_time = time.time()

# 定义参数网格
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, -1],
    'num_leaves': [31, 50, 100],
    'min_child_samples': [20, 30, 50],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# 计算总参数组合数
total_combinations = 1
for param in param_grid.values():
    total_combinations *= len(param)
print(f"参数组合: {total_combinations} 种 (3折交叉验证)")

# 三折交叉验证
cv = KFold(n_splits=3, shuffle=True, random_state=42)

# 手动网格搜索
param_combinations = list(itertools.product(*param_grid.values()))
best_score = float('-inf')
best_params = None
all_results = []

for i, values in enumerate(param_combinations):
    params = dict(zip(param_grid.keys(), values))
    model = lgb.LGBMRegressor(objective='regression', verbose=-1, random_state=42,** params)
    
    fold_scores = []
    for train_idx, val_idx in cv.split(X_train_val):
        X_fold_train, X_fold_val = X_train_val[train_idx], X_train_val[val_idx]
        y_fold_train, y_fold_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]
        
        model.fit(X_fold_train, y_fold_train)
        y_pred = model.predict(X_fold_val)
        fold_scores.append(-mean_squared_error(y_fold_val, y_pred))  # 负MSE
    
    mean_score = np.mean(fold_scores)
    all_results.append({**params, 'mean_cv_score': mean_score})
    
    if mean_score > best_score:
        best_score, best_params = mean_score, params
    
    # 每10个组合更新一次进度
    if (i + 1) % 10 == 0 or i + 1 == total_combinations:
        print(f"进度: {i + 1}/{total_combinations} | 当前最佳MSE: {np.sqrt(-best_score):.4f}")

print(f"网格搜索最佳参数: {best_params}")
print(f"最佳RMSE: {np.sqrt(-best_score):.4f}")
print(f"耗时: {time.time() - start_time:.2f}秒")

# 保存网格搜索结果
grid_results_df = pd.DataFrame(all_results).sort_values('mean_cv_score', ascending=False)
grid_results_df.to_excel(os.path.join(output_dir, "LightGBM_网格搜索结果.xlsx"), index=False)

# 网格搜索模型训练与评估
print("\n5. 网格搜索模型评估")
grid_model = lgb.LGBMRegressor(objective='regression', verbose=-1, random_state=42,** best_params)
evals_result_grid = {}
grid_model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],
    eval_names=['训练集', '验证集'],
    eval_metric='rmse',
    callbacks=[lgb.record_evaluation(evals_result_grid)]
)

# 预测与评估
y_train_pred_grid = grid_model.predict(X_train_scaled)
y_val_pred_grid = grid_model.predict(X_val_scaled)
y_test_pred_grid = grid_model.predict(X_test_scaled)


# 简化评估函数
def evaluate_model(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    print(f"{dataset_name}: R²={r2:.4f} | MAE={mae:.4f} | RMSE={rmse:.4f}")
    return {"R²": r2, "MAE": mae, "RMSE": rmse}


# 评估网格搜索模型
train_metrics_grid = evaluate_model(y_train, y_train_pred_grid, "训练集")
val_metrics_grid = evaluate_model(y_val, y_val_pred_grid, "验证集")
test_metrics_grid = evaluate_model(y_test, y_test_pred_grid, "测试集")

# 保存评估指标
grid_metrics_df = pd.DataFrame({
    "指标": ["R²", "MAE", "RMSE"],
    "训练集": [train_metrics_grid["R²"], train_metrics_grid["MAE"], train_metrics_grid["RMSE"]],
    "验证集": [val_metrics_grid["R²"], val_metrics_grid["MAE"], val_metrics_grid["RMSE"]],
    "测试集": [test_metrics_grid["R²"], test_metrics_grid["MAE"], test_metrics_grid["RMSE"]]
})
grid_metrics_df.to_excel(os.path.join(output_dir, "LightGBM_网格搜索_评估指标.xlsx"), index=False)

# Optuna精调
print("\n6. Optuna精调")
start_time = time.time()
cv_optuna = KFold(n_splits=5, shuffle=True, random_state=42)


# 防过拟合目标函数
def objective_anti_overfitting(trial):
    boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss'])
    
    params = {
        'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'random_state': 42,
        'boosting_type': boosting_type,
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        'num_leaves': trial.suggest_int('num_leaves', 10, 100),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),
        'max_bin': trial.suggest_int('max_bin', 100, 300),
        'min_split_gain': trial.suggest_float('min_split_gain', 0.1, 2.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 50.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 50.0, log=True),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),
    }

    if boosting_type == 'dart':
        params['drop_rate'] = trial.suggest_float('drop_rate', 0.1, 0.5)
        params['max_drop'] = trial.suggest_int('max_drop', 5, 30)
        params['skip_drop'] = trial.suggest_float('skip_drop', 0.3, 0.8)
    elif boosting_type == 'goss':
        top_rate = trial.suggest_float('top_rate', 0.1, 0.5)
        params['top_rate'] = top_rate
        params['other_rate'] = trial.suggest_float('other_rate', 0.05, 1.0 - top_rate)
        del params['bagging_fraction'], params['bagging_freq']

    cv_scores = []
    for train_idx, val_idx in cv_optuna.split(X_train_val):
        X_fold_train, X_fold_val = X_train_val[train_idx], X_train_val[val_idx]
        y_fold_train, y_fold_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]
        
        model = lgb.LGBMRegressor(** params)
        if boosting_type == 'dart':
            model.fit(X_fold_train, y_fold_train)
            y_pred = model.predict(X_fold_val)
        else:
            model.fit(
                X_fold_train, y_fold_train,
                eval_set=[(X_fold_val, y_fold_val)],
                eval_metric='rmse',
                callbacks=[lgb.early_stopping(30, verbose=False)]
            )
            y_pred = model.predict(X_fold_val, num_iteration=model.best_iteration_)
            
        cv_scores.append(mean_squared_error(y_fold_val, y_pred))
    
    return np.mean(cv_scores)


# 执行优化
study = optuna.create_study(
    direction='minimize',
    sampler=optuna.samplers.TPESampler(seed=42, multivariate=True, n_startup_trials=50)
)
study.optimize(objective_anti_overfitting, n_trials=200, show_progress_bar=True)

print(f"Optuna最佳参数: {study.best_params}")
print(f"最佳RMSE: {np.sqrt(study.best_value):.6f}")
print(f"耗时: {time.time() - start_time:.2f}秒")

# 保存Optuna结果
optuna_results = pd.DataFrame()
for trial in study.trials:
    optuna_results = pd.concat([optuna_results, pd.DataFrame([{**trial.params, 'value': trial.value}])], ignore_index=True)
optuna_results.to_excel(os.path.join(output_dir, "LightGBM_Optuna优化结果.xlsx"), index=False)

# 可视化Optuna结果
try:
    plot_optimization_history(study).write_image(os.path.join(output_dir, "LightGBM_Optuna优化历史.png"))
    plot_param_importances(study).write_image(os.path.join(output_dir, "LightGBM_Optuna参数重要性.png"))
    print("Optuna可视化已保存")
except Exception as e:
    print(f"可视化失败: {e}")

# 训练最终模型
print("\n7. 训练最终模型")
final_params = study.best_params.copy()
if final_params.get('boosting_type') == 'goss':
    final_params.pop('bagging_fraction', None)
    final_params.pop('bagging_freq', None)

final_model = lgb.LGBMRegressor(** final_params)
evals_result_optuna = {}

if final_params.get('boosting_type') == 'dart':
    final_model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],
        eval_names=['训练集', '验证集'],
        eval_metric='rmse',
        callbacks=[lgb.record_evaluation(evals_result_optuna)]
    )
else:
    final_model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],
        eval_names=['训练集', '验证集'],
        eval_metric='rmse',
        callbacks=[lgb.early_stopping(30, verbose=False), lgb.record_evaluation(evals_result_optuna)]
    )

# 保存模型
joblib.dump(grid_model, os.path.join(output_dir, "LightGBM_网格搜索模型.pkl"))
joblib.dump(final_model, os.path.join(output_dir, "LightGBM_Optuna最优模型.pkl"))
joblib.dump(scaler, os.path.join(output_dir, "LightGBM_特征缩放器.pkl"))

# 最终模型评估
print("\n8. 最终模型评估")
y_train_pred_optuna = final_model.predict(X_train_scaled)
y_val_pred_optuna = final_model.predict(X_val_scaled)
y_test_pred_optuna = final_model.predict(X_test_scaled)

train_metrics_optuna = evaluate_model(y_train, y_train_pred_optuna, "训练集")
val_metrics_optuna = evaluate_model(y_val, y_val_pred_optuna, "验证集")
test_metrics_optuna = evaluate_model(y_test, y_test_pred_optuna, "测试集")

# 保存评估指标
optuna_metrics_df = pd.DataFrame({
    "指标": ["R²", "MAE", "RMSE"],
    "训练集": [train_metrics_optuna["R²"], train_metrics_optuna["MAE"], train_metrics_optuna["RMSE"]],
    "验证集": [val_metrics_optuna["R²"], val_metrics_optuna["MAE"], val_metrics_optuna["RMSE"]],
    "测试集": [test_metrics_optuna["R²"], test_metrics_optuna["MAE"], test_metrics_optuna["RMSE"]]
})
optuna_metrics_df.to_excel(os.path.join(output_dir, "LightGBM_Optuna_评估指标.xlsx"), index=False)

# 绘制损失曲线
print("\n9. 生成可视化结果")
def plot_training_curve(evals_result, title, filename):
    plt.figure(figsize=(10, 6))
    x_axis = range(len(evals_result['训练集']['rmse']))
    plt.plot(x_axis, evals_result['训练集']['rmse'], 'b-', label='训练集')
    plt.plot(x_axis, evals_result['验证集']['rmse'], 'r-', label='验证集')
    plt.legend(), plt.xlabel('迭代次数'), plt.ylabel('RMSE'), plt.title(title)
    plt.savefig(filename, dpi=300, bbox_inches='tight'), plt.close()

plot_training_curve(evals_result_grid, "网格搜索模型损失曲线",
                    os.path.join(output_dir, "LightGBM_网格搜索_损失曲线.png"))
plot_training_curve(evals_result_optuna, "Optuna优化模型损失曲线",
                    os.path.join(output_dir, "LightGBM_Optuna_损失曲线.png"))

# 绘制预测对比图
def plot_actual_vs_predicted(y_true, y_pred, title, filename):
    plt.figure(figsize=(10, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    min_val, max_val = min(min(y_true), min(y_pred)), max(max(y_true), max(y_pred))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--')
    plt.xlabel('实际值'), plt.ylabel('预测值'), plt.title(title)
    plt.savefig(filename, dpi=300, bbox_inches='tight'), plt.close()

# 网格搜索模型对比图
plot_actual_vs_predicted(y_train, y_train_pred_grid, "网格搜索-训练集",
                         os.path.join(output_dir, "LightGBM_网格搜索_训练集_实际vs预测.png"))
plot_actual_vs_predicted(y_test, y_test_pred_grid, "网格搜索-测试集",
                         os.path.join(output_dir, "LightGBM_网格搜索_测试集_实际vs预测.png"))

# Optuna模型对比图
plot_actual_vs_predicted(y_train, y_train_pred_optuna, "Optuna-训练集",
                         os.path.join(output_dir, "LightGBM_Optuna_训练集_实际vs预测.png"))
plot_actual_vs_predicted(y_test, y_test_pred_optuna, "Optuna-测试集",
                         os.path.join(output_dir, "LightGBM_Optuna_测试集_实际vs预测.png"))

# 特征重要性分析
def plot_feature_importance(importance, features, title, filename):
    df = pd.DataFrame({'特征': features, '重要性': importance}).sort_values('重要性', ascending=False)
    plt.figure(figsize=(10, 6))
    plt.barh(df['特征'], df['重要性']), plt.xlabel('重要性'), plt.title(title)
    plt.savefig(filename, dpi=300, bbox_inches='tight'), plt.close()
    return df

# 网格搜索模型特征重要性
grid_imp_df = plot_feature_importance(grid_model.feature_importances_, features, 
                                      "网格搜索模型特征重要性", 
                                      os.path.join(output_dir, "LightGBM_网格搜索_特征重要性.png"))
grid_imp_df.to_excel(os.path.join(output_dir, "LightGBM_网格搜索_特征重要性.xlsx"), index=False)

# Optuna模型特征重要性
optuna_imp_df = plot_feature_importance(final_model.feature_importances_, features,
                                        "Optuna优化模型特征重要性",
                                        os.path.join(output_dir, "LightGBM_Optuna_特征重要性.png"))
optuna_imp_df.to_excel(os.path.join(output_dir, "LightGBM_Optuna_特征重要性.xlsx"), index=False)

# 保存预测结果
def save_predictions(X_data, y_true, y_pred, filename, dataset_name):
    result_df = pd.DataFrame(X_data, columns=features)
    result_df[target] = y_true.values
    result_df['预测值'] = y_pred
    result_df['残差'] = result_df[target] - result_df['预测值']
    result_df.to_excel(filename, index=False)

save_predictions(X_train, y_train, y_train_pred_optuna,
                 os.path.join(output_dir, "LightGBM_Optuna_训练集_预测结果.xlsx"), "训练集")
save_predictions(X_test, y_test, y_test_pred_optuna,
                 os.path.join(output_dir, "LightGBM_Optuna_测试集_预测结果.xlsx"), "测试集")

# 创建最终比较表
comparison_df = pd.DataFrame({
    "指标": ["R²", "MAE", "RMSE"],
    "网格搜索-训练集": [train_metrics_grid["R²"], train_metrics_grid["MAE"], train_metrics_grid["RMSE"]],
    "网格搜索-测试集": [test_metrics_grid["R²"], test_metrics_grid["MAE"], test_metrics_grid["RMSE"]],
    "Optuna-训练集": [train_metrics_optuna["R²"], train_metrics_optuna["MAE"], train_metrics_optuna["RMSE"]],
    "Optuna-测试集": [test_metrics_optuna["R²"], test_metrics_optuna["MAE"], test_metrics_optuna["RMSE"]]
})
comparison_df.to_excel(os.path.join(output_dir, "LightGBM_网格搜索vs_Optuna比较.xlsx"), index=False)

# 过拟合监控
print("\n10. 过拟合分析")
def monitor_overfitting(train, val, test):
    gaps = {
        "训练-验证差距": train["R²"] - val["R²"],
        "训练-测试差距": train["R²"] - test["R²"],
        "泛化差距": abs(val["R²"] - test["R²"])
    }
    print(f"训练-验证差距: {gaps['训练-验证差距']:.4f}")
    print(f"泛化差距: {gaps['泛化差距']:.4f}")
    return gaps

overfitting_gaps = monitor_overfitting(train_metrics_optuna, val_metrics_optuna, test_metrics_optuna)
pd.DataFrame([overfitting_gaps]).to_excel(os.path.join(output_dir, "LightGBM_过拟合分析.xlsx"), index=False)

print(f"\n所有结果已保存到: {output_dir}")
print("LightGBM模型训练完成")
