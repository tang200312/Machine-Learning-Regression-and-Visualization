import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1_l2
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, KFold
from scikeras.wrappers import KerasRegressor
import optuna
from optuna.integration import TFKerasPruningCallback
import warnings
warnings.filterwarnings('ignore')

# 设置随机种子，确保结果可复现
np.random.seed(42)
tf.random.set_seed(42)

# 创建输出目录
output_dir = r"F:\Machine leaning_SHAP\LightGBM"
os.makedirs(output_dir, exist_ok=True)

# 读取数据
print("正在读取数据...")
train_data = pd.read_excel(r"F:\Machine leaning_SHAP\train.xlsx")
validation_data = pd.read_excel(r"F:\Machine leaning_SHAP\validation.xlsx")
test_data = pd.read_excel(r"F:\Machine leaning_SHAP\test.xlsx")

# 定义特征和目标变量
features = ["TA", "RH", "P", "SP", "SD", "ATM", "PAR", "TS", "shallow_SWC", "deep_SWC"]
target = "NEP"

# 数据预处理函数
def preprocess_data(data):
    # 提取特征和目标变量
    X = data[features].values
    y = data[target].values.reshape(-1, 1)
    return X, y

# 归一化处理
print("正在进行数据归一化...")
X_scaler = MinMaxScaler(feature_range=(0, 1))
y_scaler = MinMaxScaler(feature_range=(0, 1))

X_train, y_train = preprocess_data(train_data)
X_val, y_val = preprocess_data(validation_data)
X_test, y_test = preprocess_data(test_data)

# 对特征进行归一化
X_train_scaled = X_scaler.fit_transform(X_train)
X_val_scaled = X_scaler.transform(X_val)
X_test_scaled = X_scaler.transform(X_test)

# 对目标变量进行归一化
y_train_scaled = y_scaler.fit_transform(y_train)
y_val_scaled = y_scaler.transform(y_val)
y_test_scaled = y_scaler.transform(y_test)

# 准备LSTM输入格式 (samples, time_steps, features)
def create_sequences(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:(i + time_steps)])
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

# 设置时间步长
time_steps = 24  # 使用前24个小时的数据预测下一个小时

# 创建序列数据
print("正在创建序列数据...")
X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)
X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, time_steps)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)

# 网格搜索粗调的最优参数
grid_search_best_params = {
    'batch_size': 64, 
    'epochs': 50, 
    'model__dropout_rate': 0.3, 
    'model__learning_rate': 0.01, 
    'model__units': 32
}

# 保存粗调参数
grid_search_params_df = pd.DataFrame([grid_search_best_params])
grid_search_params_df.to_excel(os.path.join(output_dir, 'lstm_grid_search_best_params.xlsx'), index=False)

# 定义创建模型的函数（用于Optuna精调）
def create_lstm_model(units, dropout_rate, learning_rate, recurrent_dropout_rate=0.0, 
                     l1_reg=0.0, l2_reg=0.0, activation='tanh', recurrent_activation='sigmoid'):
    model = Sequential()
    model.add(LSTM(units=units, 
                  return_sequences=True, 
                  input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
                  dropout=dropout_rate,
                  recurrent_dropout=recurrent_dropout_rate,
                  kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),
                  activation=activation,
                  recurrent_activation=recurrent_activation))
    
    model.add(LSTM(units=units//2, 
                  dropout=dropout_rate,
                  recurrent_dropout=recurrent_dropout_rate,
                  kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),
                  activation=activation,
                  recurrent_activation=recurrent_activation))
    
    model.add(Dense(1, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# 定义Optuna目标函数
def objective(trial):
    # 定义超参数搜索空间
    units = trial.suggest_categorical('units', [16, 24, 32, 48, 64])
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.05)
    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01, log=True)
    batch_size = trial.suggest_categorical('batch_size', [32, 48, 64, 96, 128])
    recurrent_dropout_rate = trial.suggest_float('recurrent_dropout_rate', 0.0, 0.3, step=0.05)
    l1_reg = trial.suggest_float('l1_reg', 1e-8, 1e-3, log=True)
    l2_reg = trial.suggest_float('l2_reg', 1e-8, 1e-3, log=True)
    activation = trial.suggest_categorical('activation', ['tanh', 'relu', 'elu'])
    recurrent_activation = trial.suggest_categorical('recurrent_activation', ['sigmoid', 'hard_sigmoid'])
    
    # 五折交叉验证
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = []
    
    for train_idx, val_idx in kf.split(X_train_seq):
        X_train_fold, X_val_fold = X_train_seq[train_idx], X_train_seq[val_idx]
        y_train_fold, y_val_fold = y_train_seq[train_idx], y_train_seq[val_idx]
        
        # 创建模型
        model = create_lstm_model(
            units=units,
            dropout_rate=dropout_rate,
            learning_rate=learning_rate,
            recurrent_dropout_rate=recurrent_dropout_rate,
            l1_reg=l1_reg,
            l2_reg=l2_reg,
            activation=activation,
            recurrent_activation=recurrent_activation
        )
        
        # 早停法
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # Optuna剪枝回调
        pruning_callback = TFKerasPruningCallback(trial, 'val_loss')
        
        # 训练模型
        history = model.fit(
            X_train_fold, y_train_fold,
            epochs=50,  # 最大轮次
            batch_size=batch_size,
            validation_data=(X_val_fold, y_val_fold),
            callbacks=[early_stopping, pruning_callback],
            verbose=0
        )
        
        # 评估模型
        val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)
        cv_scores.append(val_loss)
        
        # 清理内存
        tf.keras.backend.clear_session()
    
    # 返回平均验证损失
    mean_val_loss = np.mean(cv_scores)
    return mean_val_loss

# 运行Optuna优化
print("正在使用Optuna进行超参数精调（200次实验）...")
study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=200, show_progress_bar=True)

# 输出最佳参数
print("Optuna精调最佳参数:")
print(study.best_params)
print(f"最佳验证损失: {study.best_value}")

# 保存Optuna精调的最佳参数
optuna_best_params_df = pd.DataFrame([study.best_params])
optuna_best_params_df.to_excel(os.path.join(output_dir, 'lstm_optuna_best_params.xlsx'), index=False)

# 使用最佳参数创建最终模型
best_units = study.best_params['units']
best_dropout = study.best_params['dropout_rate']
best_learning_rate = study.best_params['learning_rate']
best_batch_size = study.best_params['batch_size']
best_recurrent_dropout = study.best_params['recurrent_dropout_rate']
best_l1_reg = study.best_params['l1_reg']
best_l2_reg = study.best_params['l2_reg']
best_activation = study.best_params['activation']
best_recurrent_activation = study.best_params['recurrent_activation']

# 创建最终模型
print("正在训练最终模型...")
final_model = create_lstm_model(
    units=best_units,
    dropout_rate=best_dropout,
    learning_rate=best_learning_rate,
    recurrent_dropout_rate=best_recurrent_dropout,
    l1_reg=best_l1_reg,
    l2_reg=best_l2_reg,
    activation=best_activation,
    recurrent_activation=best_recurrent_activation
)

# 添加早停和模型检查点回调，防止过拟合
early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath=os.path.join(output_dir, 'best_lstm_model.h5'),
    monitor='val_loss',
    save_best_only=True
)

# 训练最终模型
history = final_model.fit(
    X_train_seq, y_train_seq,
    epochs=100,  # 设置较大的轮次，让早停机制决定何时停止
    batch_size=best_batch_size,
    validation_data=(X_val_seq, y_val_seq),
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

# 绘制损失曲线
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='训练损失')
plt.plot(history.history['val_loss'], label='验证损失')
plt.title('模型损失')
plt.ylabel('损失')
plt.xlabel('轮次')
plt.legend()
plt.savefig(os.path.join(output_dir, 'lstm_loss_curve.png'))
plt.close()

# 模型预测
def predict_and_evaluate(model, X_seq, y_true_scaled, dataset_name):
    # 预测
    y_pred_scaled = model.predict(X_seq)
    
    # 反归一化
    y_true = y_scaler.inverse_transform(y_true_scaled)
    y_pred = y_scaler.inverse_transform(y_pred_scaled)
    
    # 计算评估指标
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    
    print(f"\n{dataset_name} 评估指标:")
    print(f"R²: {r2:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    
    # 保存评估指标
    metrics_df = pd.DataFrame({
        'Dataset': [dataset_name],
        'R2': [r2],
        'MAE': [mae],
        'MSE': [mse],
        'RMSE': [rmse]
    })
    
    # 保存预测结果
    results_df = pd.DataFrame({
        'True_Values': y_true.flatten(),
        'Predicted_Values': y_pred.flatten()
    })
    
    return metrics_df, results_df

# 评估模型性能
print("正在评估模型性能...")
train_metrics, train_results = predict_and_evaluate(final_model, X_train_seq, y_train_seq, "训练集")
val_metrics, val_results = predict_and_evaluate(final_model, X_val_seq, y_val_seq, "验证集")
test_metrics, test_results = predict_and_evaluate(final_model, X_test_seq, y_test_seq, "测试集")

# 合并所有评估指标
all_metrics = pd.concat([train_metrics, val_metrics, test_metrics])

# 保存评估指标和预测结果
all_metrics.to_excel(os.path.join(output_dir, 'lstm_metrics.xlsx'), index=False)
train_results.to_excel(os.path.join(output_dir, 'lstm_train_predictions.xlsx'), index=False)
val_results.to_excel(os.path.join(output_dir, 'lstm_validation_predictions.xlsx'), index=False)
test_results.to_excel(os.path.join(output_dir, 'lstm_test_predictions.xlsx'), index=False)

# 绘制预测值与真实值的对比图
def plot_predictions(true_values, pred_values, dataset_name):
    plt.figure(figsize=(12, 6))
    plt.plot(true_values, label='真实值', alpha=0.7)
    plt.plot(pred_values, label='预测值', alpha=0.7)
    plt.title(f'{dataset_name} 预测结果对比')
    plt.xlabel('样本')
    plt.ylabel('NEP')
    plt.legend()
    plt.savefig(os.path.join(output_dir, f'lstm_{dataset_name}_predictions.png'))
    plt.close()

# 绘制预测结果对比图
plot_predictions(train_results['True_Values'], train_results['Predicted_Values'], '训练集')
plot_predictions(val_results['True_Values'], val_results['Predicted_Values'], '验证集')
plot_predictions(test_results['True_Values'], test_results['Predicted_Values'], '测试集')

# 绘制散点图
def plot_scatter(true_values, pred_values, dataset_name):
    plt.figure(figsize=(8, 8))
    plt.scatter(true_values, pred_values, alpha=0.5)
    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], 'r--')
    plt.title(f'{dataset_name} 真实值 vs 预测值')
    plt.xlabel('真实值')
    plt.ylabel('预测值')
    plt.savefig(os.path.join(output_dir, f'lstm_{dataset_name}_scatter.png'))
    plt.close()

# 绘制散点图
plot_scatter(train_results['True_Values'], train_results['Predicted_Values'], '训练集')
plot_scatter(val_results['True_Values'], val_results['Predicted_Values'], '验证集')
plot_scatter(test_results['True_Values'], test_results['Predicted_Values'], '测试集')

# 保存超参数调优历史
optuna_history = []
for trial in study.trials:
    if trial.state == optuna.trial.TrialState.COMPLETE:
        params = trial.params.copy()
        params['value'] = trial.value
        optuna_history.append(params)

optuna_history_df = pd.DataFrame(optuna_history)
optuna_history_df.to_excel(os.path.join(output_dir, 'lstm_optuna_history.xlsx'), index=False)

# 比较粗调和精调结果
comparison_df = pd.DataFrame({
    '调优方法': ['网格搜索粗调', 'Optuna精调'],
    '单元数': [grid_search_best_params['model__units'], study.best_params['units']],
    'Dropout率': [grid_search_best_params['model__dropout_rate'], study.best_params['dropout_rate']],
    '学习率': [grid_search_best_params['model__learning_rate'], study.best_params['learning_rate']],
    '批量大小': [grid_search_best_params['batch_size'], study.best_params['batch_size']],
    '循环Dropout率': ['N/A', study.best_params['recurrent_dropout_rate']],
    'L1正则化': ['N/A', study.best_params['l1_reg']],
    'L2正则化': ['N/A', study.best_params['l2_reg']],
    '激活函数': ['tanh', study.best_params['activation']],
    '循环激活函数': ['sigmoid', study.best_params['recurrent_activation']]
})

comparison_df.to_excel(os.path.join(output_dir, 'lstm_hyperparameter_comparison.xlsx'), index=False)

print(f"\n所有结果已保存到 {output_dir}")
