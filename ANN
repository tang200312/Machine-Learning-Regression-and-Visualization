import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.regularizers import l1, l2, l1_l2
from scikeras.wrappers import KerasRegressor
import tensorflow as tf
import warnings
import optuna
import joblib
import json

warnings.filterwarnings('ignore')

# 随机种子固定
np.random.seed(42)
tf.random.set_seed(42)

# 路径配置
output_dir = "F:\\Machine leaning_SHAP\\KNN2"
os.makedirs(output_dir, exist_ok=True)
data_path = "F:\\Machine leaning_SHAP\\数据集.xlsx"


# 数据加载与划分
def load_and_split_data(file_path):
    try:
        data = pd.read_excel(file_path)
        print(f"1. 加载数据: {file_path} (总量: {len(data)})")

        features = ["TA", "RH", "P", "SP", "SD", "ATM", "PAR", "TS", "shallow_SWC", "deep_SWC"]
        target = "NEP"
        missing_cols = [col for col in features + [target] if col not in data.columns]
        if missing_cols:
            raise ValueError(f"缺失列: {missing_cols}")

        data_clean = data[features + [target]].dropna()
        print(f"清洗后数据量: {len(data_clean)}")

        data_shuffled = data_clean.sample(frac=1, random_state=42).reset_index(drop=True)
        train_size, val_size, test_size = 21888, 7344, 5856
        required_total = train_size + val_size + test_size

        if len(data_shuffled) < required_total:
            ratio = len(data_shuffled) / required_total
            train_size = int(train_size * ratio)
            val_size = int(val_size * ratio)
            test_size = len(data_shuffled) - train_size - val_size
            print(f"数据不足，调整划分: 训练集{train_size}, 验证集{val_size}, 测试集{test_size}")

        train_data = data_shuffled[:train_size]
        val_data = data_shuffled[train_size:train_size + val_size]
        test_data = data_shuffled[train_size + val_size:train_size + val_size + test_size]

        print(f"划分完成: 训练集{len(train_data)} | 验证集{len(val_data)} | 测试集{len(test_data)}")
        return train_data, val_data, test_data, features

    except Exception as e:
        print(f"数据加载失败: {e}")
        return None, None, None, None


# 数据预处理
def preprocess_data(train_data, val_data, test_data, features):
    target = "NEP"
    # 提取特征与目标
    X_train, y_train = train_data[features].values, train_data[target].values
    X_val, y_val = val_data[features].values, val_data[target].values
    X_test, y_test = test_data[features].values, test_data[target].values

    # 特征归一化
    scaler_X = MinMaxScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    X_test_scaled = scaler_X.transform(X_test)

    # 目标归一化
    scaler_y = MinMaxScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

    return (X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled,
            X_test_scaled, y_test_scaled, scaler_X, scaler_y)


# ANN模型构建
def create_ann_model(neurons=128, hidden_layers=3, dropout_rate=0.1, learning_rate=0.001,
                     activation='relu', kernel_initializer='he_normal',
                     kernel_regularizer=None, l1_ratio=0.01, l2_ratio=0.01,
                     use_batch_norm=False):
    model = Sequential()
    # 正则化器设置
    if kernel_regularizer == 'l1':
        regularizer = l1(l1_ratio)
    elif kernel_regularizer == 'l2':
        regularizer = l2(l2_ratio)
    elif kernel_regularizer == 'l1_l2':
        regularizer = l1_l2(l1=l1_ratio, l2=l2_ratio)
    else:
        regularizer = None

    # 输入层
    model.add(Dense(neurons, activation=activation, input_shape=(10,),
                    kernel_initializer=kernel_initializer, kernel_regularizer=regularizer))
    if use_batch_norm:
        model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    # 隐藏层
    for _ in range(hidden_layers - 1):
        model.add(Dense(neurons, activation=activation,
                        kernel_initializer=kernel_initializer, kernel_regularizer=regularizer))
        if use_batch_norm:
            model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

    # 输出层
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])
    return model


# 模型评估
def evaluate_model(y_true, y_pred, scaler_y=None):
    if scaler_y:
        y_true = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()
        y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()
    return {
        'R²': r2_score(y_true, y_pred),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))
    }


# Optuna交叉验证目标函数
def objective_cv(trial, X, y, n_splits=5):
    params = {
        'model__neurons': trial.suggest_categorical('neurons', [64, 128, 256, 512]),
        'model__hidden_layers': trial.suggest_int('hidden_layers', 2, 5),
        'model__dropout_rate': trial.suggest_float('dropout_rate', 0.0, 0.5),
        'model__learning_rate': trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True),
        'model__activation': trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'tanh']),
        'model__kernel_initializer': trial.suggest_categorical('kernel_initializer',
                                                               ['he_normal', 'he_uniform', 'glorot_normal', 'glorot_uniform']),
        'model__kernel_regularizer': trial.suggest_categorical('kernel_regularizer', [None, 'l1', 'l2', 'l1_l2']),
        'model__l1_ratio': trial.suggest_float('l1_ratio', 1e-5, 1e-2, log=True),
        'model__l2_ratio': trial.suggest_float('l2_ratio', 1e-5, 1e-2, log=True),
        'model__use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),
        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
        'epochs': 100
    }

    model = KerasRegressor(model=create_ann_model, verbose=0)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in kf.split(X):
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]

        for param_name, param_value in params.items():
            setattr(model, param_name, param_value)

        model.fit(X_train_fold, y_train_fold,
                  validation_data=(X_val_fold, y_val_fold),
                  callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],
                  verbose=0)

        y_pred = model.predict(X_val_fold)
        scores.append(mean_squared_error(y_val_fold, y_pred))

    return np.mean(scores)


# 主函数
def main():
    # 1. 数据加载与预处理
    train_data, val_data, test_data, features = load_and_split_data(data_path)
    if not all([train_data, val_data, test_data]):
        print("程序终止")
        return

    (X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled,
     X_test_scaled, y_test_scaled, scaler_X, scaler_y) = preprocess_data(train_data, val_data, test_data, features)

    print(f"2. 预处理完成: 训练集{X_train_scaled.shape[0]} | 验证集{X_val_scaled.shape[0]} | 测试集{X_test_scaled.shape[0]}")

    # 保存划分数据
    train_data.to_excel(os.path.join(output_dir, 'train_dataset.xlsx'), index=False)
    val_data.to_excel(os.path.join(output_dir, 'validation_dataset.xlsx'), index=False)
    test_data.to_excel(os.path.join(output_dir, 'test_dataset.xlsx'), index=False)

    # 3. 网格搜索基准参数
    grid_best_params = {
        'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.1,
        'model__hidden_layers': 3, 'model__learning_rate': 0.001, 'model__neurons': 128
    }
    print(f"3. 网格搜索最佳参数: {grid_best_params}")
    with open(os.path.join(output_dir, 'grid_search_best_params.json'), 'w') as f:
        json.dump(grid_best_params, f, indent=4)

    # 4. Optuna调优
    X_train_val = np.vstack((X_train_scaled, X_val_scaled))
    y_train_val = np.concatenate((y_train_scaled, y_val_scaled))
    print("4. Optuna调优: 5折交叉验证，200次试验")

    study = optuna.create_study(direction='minimize', study_name='ann_opt', pruner=optuna.pruners.MedianPruner())
    study.optimize(lambda trial: objective_cv(trial, X_train_val, y_train_val),
                   n_trials=200, show_progress_bar=True)

    opt_best_params = study.best_params
    print(f"Optuna最佳参数: {opt_best_params}")
    with open(os.path.join(output_dir, 'optuna_best_params.json'), 'w') as f:
        json.dump(opt_best_params, f, indent=4)
    joblib.dump(study, os.path.join(output_dir, 'optuna_study.pkl'))

    # 5. 训练最终模型
    print("5. 训练最终模型")
    final_model = create_ann_model(
        neurons=opt_best_params.get('neurons', 128),
        hidden_layers=opt_best_params.get('hidden_layers', 3),
        dropout_rate=opt_best_params.get('dropout_rate', 0.1),
        learning_rate=opt_best_params.get('learning_rate', 0.001),
        activation=opt_best_params.get('activation', 'relu'),
        kernel_initializer=opt_best_params.get('kernel_initializer', 'he_normal'),
        kernel_regularizer=opt_best_params.get('kernel_regularizer'),
        l1_ratio=opt_best_params.get('l1_ratio', 0.01),
        l2_ratio=opt_best_params.get('l2_ratio', 0.01),
        use_batch_norm=opt_best_params.get('use_batch_norm', False)
    )

    callbacks = [
        EarlyStopping(monitor='val_loss', patience=opt_best_params.get('patience', 20), restore_best_weights=True),
        ModelCheckpoint(os.path.join(output_dir, 'best_ann_model.h5'), monitor='val_loss', save_best_only=True)
    ]

    history = final_model.fit(
        X_train_scaled, y_train_scaled,
        validation_data=(X_val_scaled, y_val_scaled),
        epochs=100, batch_size=opt_best_params.get('batch_size', 64),
        callbacks=callbacks, verbose=1
    )

    # 6. 可视化损失曲线
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='训练损失')
    plt.plot(history.history['val_loss'], label='验证损失')
    plt.title('损失曲线')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='训练MAE')
    plt.plot(history.history['val_mae'], label='验证MAE')
    plt.title('MAE曲线')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ann_loss_curves.png'))
    plt.close()

    # 7. 模型预测与评估
    y_train_pred = final_model.predict(X_train_scaled)
    y_val_pred = final_model.predict(X_val_scaled)
    y_test_pred = final_model.predict(X_test_scaled)

    train_metrics = evaluate_model(y_train_scaled, y_train_pred, scaler_y)
    val_metrics = evaluate_model(y_val_scaled, y_val_pred, scaler_y)
    test_metrics = evaluate_model(y_test_scaled, y_test_pred, scaler_y)

    print("\n6. 模型评估:")
    print(f"训练集: R²{train_metrics['R²']:.4f} | MAE{train_metrics['MAE']:.4f} | MSE{train_metrics['MSE']:.4f} | RMSE{train_metrics['RMSE']:.4f}")
    print(f"验证集: R²{val_metrics['R²']:.4f} | MAE{val_metrics['MAE']:.4f} | MSE{val_metrics['MSE']:.4f} | RMSE{val_metrics['RMSE']:.4f}")
    print(f"测试集: R²{test_metrics['R²']:.4f} | MAE{test_metrics['MAE']:.4f} | MSE{test_metrics['MSE']:.4f} | RMSE{test_metrics['RMSE']:.4f}")

    # 8. 保存结果
    # 评估指标
    metrics_df = pd.DataFrame({
        'Metric': list(train_metrics.keys()),
        'Training': list(train_metrics.values()),
        'Validation': list(val_metrics.values()),
        'Testing': list(test_metrics.values())
    })
    metrics_df.to_csv(os.path.join(output_dir, 'ann_metrics.csv'), index=False)

    # 网格搜索模型对比
    grid_model = create_ann_model(
        neurons=grid_best_params['model__neurons'],
        hidden_layers=grid_best_params['model__hidden_layers'],
        dropout_rate=grid_best_params['model__dropout_rate'],
        learning_rate=grid_best_params['model__learning_rate']
    )
    grid_model.fit(X_train_scaled, y_train_scaled,
                   validation_data=(X_val_scaled, y_val_scaled),
                   epochs=grid_best_params['epochs'], batch_size=grid_best_params['batch_size'],
                   callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],
                   verbose=0)

    grid_train_pred = grid_model.predict(X_train_scaled)
    grid_val_pred = grid_model.predict(X_val_scaled)
    grid_test_pred = grid_model.predict(X_test_scaled)

    grid_train_metrics = evaluate_model(y_train_scaled, grid_train_pred, scaler_y)
    grid_val_metrics = evaluate_model(y_val_scaled, grid_val_pred, scaler_y)
    grid_test_metrics = evaluate_model(y_test_scaled, grid_test_pred, scaler_y)

    tuning_comp = pd.DataFrame({
        'Metric': list(train_metrics.keys
